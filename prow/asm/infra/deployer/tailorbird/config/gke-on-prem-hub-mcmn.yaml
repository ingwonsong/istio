apiVersion: sut.tailorbird.io/v1alpha1
kind: Rookery
metadata:
  # !!!IMPORTANT!!!! READ BELOW
  # Use names that are unique and easily identifiable
  # To see list of currently running rookeries:
  # kubectl get rookeries -n tailorbird-users
  # To view existing knests:
  # kubectl get knests -n tailorbird-users
  # To view existing clusters:
  # kubectl get clusters.sut.tailorbird.io -n tailorbird-users
  # e.g., use your Google username as prefix.
  # !!!IMPORTANT!!!! READ ABOVE
  name: asm-ci-onprem-hub-mcmn
  namespace: tailorbird-users
spec:
  ttlSeconds: 10800 # 3 hours
  knests:
    - apiVersion: sut.tailorbird.io/v1alpha1
      kind: Knest
      metadata:
        name: asm-ci-onprem-hub-mcmn-knest
      spec:
        clusters:
          # TODO(tairan): use two clusters after b/176177944 is fixed
          - apiVersion: sut.tailorbird.io/v1alpha1
            kind: Cluster
            metadata:
              name: asm-ci-onprem-hub-mcmn-cluster-01
            spec:
              provider: vsphere
              distribution: gke
              provisionerArgs:
                username: prowuser
                workerNumNodes: "4"
                {{- if .Version }}
                gkectlVersion: "{{ .Version }}"
                {{- end }}
                {{- if .VersionPrefix }}
                gkectlVersionPrefix: "{{ .VersionPrefix }}"
                releaseMode: "true"
                useStagingBucket: "true"
                {{- end }}
                # workaround for b/181781346
                gkectlConfigSeedVersion: "v1"
                vipNetwork: "fe"
                # Assign the first cluster under one VPC
                vsphereVpc: "atl-shared-vpc-02"
                networkType: "any"
                # useVipNetworkLabel requires use=frontent and i-know-what-i-am-doing=true
                # if you dont know what you're doing, the request will fail (it knows).
                useVipNetworkLabel: "use=frontend,i-know-what-i-am-doing=true"
                # useNetworkLabel requires use=private and i-know-what-i-am-doing=true
                useNetworkLabel: "use=private,i-know-what-i-am-doing=true"
                gkeConnectProject: "tairan-asm-multi-cloud-dev"
                stackdriverProjectId: "tairan-asm-multi-cloud-dev"
                vsphereVersion: "7.0"
                esxiVersion: "7.0"
                herculesEnvTtl: "4h"
                # default is 192.168.0.0/16 and 10.96.232.0/24, we split pod subnet in two and use lower subnet
                userExtraConfig: "network.podCIDR=192.168.0.0/17,network.serviceCIDR=10.96.232.0/24"
          - apiVersion: sut.tailorbird.io/v1alpha1
            kind: Cluster
            metadata:
              name: asm-ci-onprem-hub-mcmn-cluster-02
            spec:
              provider: vsphere
              distribution: gke
              provisionerArgs:
                username: prowuser
                workerNumNodes: "4"
                {{- if .Version }}
                gkectlVersion: "{{ .Version }}"
                {{- end }}
                {{- if .VersionPrefix }}
                gkectlVersionPrefix: "{{ .VersionPrefix }}"
                releaseMode: "true"
                useStagingBucket: "true"
                {{- end }}
                # workaround for b/181781346
                gkectlConfigSeedVersion: "v1"
                vipNetwork: "fe"
                # Assign the first cluster under one VPC
                vsphereVpc: "atl-shared-vpc-02"
                networkType: "any"
                # useVipNetworkLabel requires use=frontent and i-know-what-i-am-doing=true
                # if you dont know what you're doing, the request will fail (it knows).
                useVipNetworkLabel: "use=frontend,i-know-what-i-am-doing=true"
                # useNetworkLabel requires use=private and i-know-what-i-am-doing=true
                useNetworkLabel: "use=private,i-know-what-i-am-doing=true"
                gkeConnectProject: "tairan-asm-multi-cloud-dev"
                stackdriverProjectId: "tairan-asm-multi-cloud-dev"
                vsphereVersion: "7.0"
                esxiVersion: "7.0"
                herculesEnvTtl: "4h"
                # default is 192.168.0.0/16 and 10.96.232.0/24, we split pod in two and use higher subnet, and use next /24 for service
                userExtraConfig: "network.podCIDR=192.168.128.0/17,network.serviceCIDR=10.96.233.0/24"
